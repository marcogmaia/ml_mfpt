{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponível. Processamento em GPU!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "usar_gpu = torch.cuda.is_available()\n",
    "# usar_gpu = False\n",
    "if usar_gpu:\n",
    "    print(\"CUDA disponível. Processamento em GPU!\")\n",
    "else:\n",
    "    print(\"CUDA NÃO disponível. Processamento em CPU!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.seq1 = nn.Sequential(nn.Linear(5, 10), nn.Sigmoid())\n",
    "        self.seq2 = nn.Sequential(nn.Linear(10, 50), nn.ReLU(inplace=True))\n",
    "        self.seq21 = nn.Sequential(nn.Linear(50, 50), nn.ReLU(inplace=True))\n",
    "        self.seq3 = nn.Sequential(nn.Linear(50, 10), nn.ReLU(inplace=True))\n",
    "        self.seq4 = nn.Sequential(nn.Linear(10, 2), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.seq1(x)\n",
    "        o = self.seq2(o)\n",
    "        o = self.seq21(o)\n",
    "        o = self.seq3(o)\n",
    "        o = self.seq4(o)\n",
    "\n",
    "        return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes: (1019, 5) (1019,)\n",
      "test shapes:  (437, 5) (437,)\n"
     ]
    }
   ],
   "source": [
    "class MFPTDataset(Dataset):\n",
    "    def __init__(self, xs, ys) -> None:\n",
    "        super().__init__()\n",
    "        self.features = xs\n",
    "        self.labels = ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "data = np.load(\"MFPT_Features.npz\")\n",
    "\n",
    "x, y = data[\"x_features\"], data[\"y_labels\"]\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=4)\n",
    "\n",
    "batch_size = 25\n",
    "train_loader = DataLoader(MFPTDataset(xtrain, ytrain), batch_size, shuffle=True)\n",
    "test_loader = DataLoader(MFPTDataset(xtest, ytest), batch_size, shuffle=True)\n",
    "\n",
    "print(\"train shapes:\", xtrain.shape, ytrain.shape)\n",
    "print(\"test shapes: \", xtest.shape, ytest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (seq1): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (seq2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=50, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (seq21): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (seq3): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (seq4): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=2, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "print(model)\n",
    "\n",
    "if usar_gpu:\n",
    "    torch.cuda.init()\n",
    "    model.cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 1/500 \t Loss de Treinamento: 0.7006053328514099\n",
      "Epoca 2/500 \t Loss de Treinamento: 0.695509135723114\n",
      "Epoca 3/500 \t Loss de Treinamento: 0.6992497444152832\n",
      "Epoca 4/500 \t Loss de Treinamento: 0.6930257678031921\n",
      "Epoca 5/500 \t Loss de Treinamento: 0.6924751400947571\n",
      "Epoca 6/500 \t Loss de Treinamento: 0.68632572889328\n",
      "Epoca 7/500 \t Loss de Treinamento: 0.6974618434906006\n",
      "Epoca 8/500 \t Loss de Treinamento: 0.6721261143684387\n",
      "Epoca 9/500 \t Loss de Treinamento: 0.6741512417793274\n",
      "Epoca 10/500 \t Loss de Treinamento: 0.6829085350036621\n",
      "Epoca 11/500 \t Loss de Treinamento: 0.6650465726852417\n",
      "Epoca 12/500 \t Loss de Treinamento: 0.6818575263023376\n",
      "Epoca 13/500 \t Loss de Treinamento: 0.6169257164001465\n",
      "Epoca 14/500 \t Loss de Treinamento: 0.6349934935569763\n",
      "Epoca 15/500 \t Loss de Treinamento: 0.6057677865028381\n",
      "Epoca 16/500 \t Loss de Treinamento: 0.6291674375534058\n",
      "Epoca 17/500 \t Loss de Treinamento: 0.5975169539451599\n",
      "Epoca 18/500 \t Loss de Treinamento: 0.6556391716003418\n",
      "Epoca 19/500 \t Loss de Treinamento: 0.5603744387626648\n",
      "Epoca 20/500 \t Loss de Treinamento: 0.5905766487121582\n",
      "Epoca 21/500 \t Loss de Treinamento: 0.7535558342933655\n",
      "Epoca 22/500 \t Loss de Treinamento: 0.6237126588821411\n",
      "Epoca 23/500 \t Loss de Treinamento: 0.5198810696601868\n",
      "Epoca 24/500 \t Loss de Treinamento: 0.6181181073188782\n",
      "Epoca 25/500 \t Loss de Treinamento: 0.6141581535339355\n",
      "Epoca 26/500 \t Loss de Treinamento: 0.6118081212043762\n",
      "Epoca 27/500 \t Loss de Treinamento: 0.5875216126441956\n",
      "Epoca 28/500 \t Loss de Treinamento: 0.6096531748771667\n",
      "Epoca 29/500 \t Loss de Treinamento: 0.6761501431465149\n",
      "Epoca 30/500 \t Loss de Treinamento: 0.6396408677101135\n",
      "Epoca 31/500 \t Loss de Treinamento: 0.5729955434799194\n",
      "Epoca 32/500 \t Loss de Treinamento: 0.5375729203224182\n",
      "Epoca 33/500 \t Loss de Treinamento: 0.6473360061645508\n",
      "Epoca 34/500 \t Loss de Treinamento: 0.5469843149185181\n",
      "Epoca 35/500 \t Loss de Treinamento: 0.5679256319999695\n",
      "Epoca 36/500 \t Loss de Treinamento: 0.6616883277893066\n",
      "Epoca 37/500 \t Loss de Treinamento: 0.6266608238220215\n",
      "Epoca 38/500 \t Loss de Treinamento: 0.5663778185844421\n",
      "Epoca 39/500 \t Loss de Treinamento: 0.519578754901886\n",
      "Epoca 40/500 \t Loss de Treinamento: 0.7727882266044617\n",
      "Epoca 41/500 \t Loss de Treinamento: 0.5630167126655579\n",
      "Epoca 42/500 \t Loss de Treinamento: 0.6741112470626831\n",
      "Epoca 43/500 \t Loss de Treinamento: 0.5642229318618774\n",
      "Epoca 44/500 \t Loss de Treinamento: 0.6185712814331055\n",
      "Epoca 45/500 \t Loss de Treinamento: 0.6221775412559509\n",
      "Epoca 46/500 \t Loss de Treinamento: 0.6839897632598877\n",
      "Epoca 47/500 \t Loss de Treinamento: 0.6265918016433716\n",
      "Epoca 48/500 \t Loss de Treinamento: 0.576567530632019\n",
      "Epoca 49/500 \t Loss de Treinamento: 0.6169568300247192\n",
      "Epoca 50/500 \t Loss de Treinamento: 0.5691487193107605\n",
      "Epoca 51/500 \t Loss de Treinamento: 0.5531526207923889\n",
      "Epoca 52/500 \t Loss de Treinamento: 0.5910747647285461\n",
      "Epoca 53/500 \t Loss de Treinamento: 0.6271824836730957\n",
      "Epoca 54/500 \t Loss de Treinamento: 0.5810831189155579\n",
      "Epoca 55/500 \t Loss de Treinamento: 0.5474992990493774\n",
      "Epoca 56/500 \t Loss de Treinamento: 0.6574662923812866\n",
      "Epoca 57/500 \t Loss de Treinamento: 0.5257110595703125\n",
      "Epoca 58/500 \t Loss de Treinamento: 0.6095229983329773\n",
      "Epoca 59/500 \t Loss de Treinamento: 0.5452280640602112\n",
      "Epoca 60/500 \t Loss de Treinamento: 0.5198541879653931\n",
      "Epoca 61/500 \t Loss de Treinamento: 0.5627247095108032\n",
      "Epoca 62/500 \t Loss de Treinamento: 0.5221686959266663\n",
      "Epoca 63/500 \t Loss de Treinamento: 0.5546478629112244\n",
      "Epoca 64/500 \t Loss de Treinamento: 0.5523657202720642\n",
      "Epoca 65/500 \t Loss de Treinamento: 0.5122073888778687\n",
      "Epoca 66/500 \t Loss de Treinamento: 0.49646928906440735\n",
      "Epoca 67/500 \t Loss de Treinamento: 0.5282719135284424\n",
      "Epoca 68/500 \t Loss de Treinamento: 0.42526209354400635\n",
      "Epoca 69/500 \t Loss de Treinamento: 0.5627529621124268\n",
      "Epoca 70/500 \t Loss de Treinamento: 0.5413350462913513\n",
      "Epoca 71/500 \t Loss de Treinamento: 0.5024719834327698\n",
      "Epoca 72/500 \t Loss de Treinamento: 0.5394726991653442\n",
      "Epoca 73/500 \t Loss de Treinamento: 0.5658271908760071\n",
      "Epoca 74/500 \t Loss de Treinamento: 0.5626671314239502\n",
      "Epoca 75/500 \t Loss de Treinamento: 0.5422092080116272\n",
      "Epoca 76/500 \t Loss de Treinamento: 0.4725620448589325\n",
      "Epoca 77/500 \t Loss de Treinamento: 0.5021396279335022\n",
      "Epoca 78/500 \t Loss de Treinamento: 0.45966264605522156\n",
      "Epoca 79/500 \t Loss de Treinamento: 0.4416508376598358\n",
      "Epoca 80/500 \t Loss de Treinamento: 0.5939570665359497\n",
      "Epoca 81/500 \t Loss de Treinamento: 0.4750843644142151\n",
      "Epoca 82/500 \t Loss de Treinamento: 0.5359596610069275\n",
      "Epoca 83/500 \t Loss de Treinamento: 0.4613354206085205\n",
      "Epoca 84/500 \t Loss de Treinamento: 0.5230767130851746\n",
      "Epoca 85/500 \t Loss de Treinamento: 0.5781099796295166\n",
      "Epoca 86/500 \t Loss de Treinamento: 0.4517883062362671\n",
      "Epoca 87/500 \t Loss de Treinamento: 0.5300315618515015\n",
      "Epoca 88/500 \t Loss de Treinamento: 0.4481099843978882\n",
      "Epoca 89/500 \t Loss de Treinamento: 0.5124562978744507\n",
      "Epoca 90/500 \t Loss de Treinamento: 0.4815801680088043\n",
      "Epoca 91/500 \t Loss de Treinamento: 0.5464027523994446\n",
      "Epoca 92/500 \t Loss de Treinamento: 0.5637964606285095\n",
      "Epoca 93/500 \t Loss de Treinamento: 0.49713030457496643\n",
      "Epoca 94/500 \t Loss de Treinamento: 0.5279805660247803\n",
      "Epoca 95/500 \t Loss de Treinamento: 0.5353702902793884\n",
      "Epoca 96/500 \t Loss de Treinamento: 0.48982545733451843\n",
      "Epoca 97/500 \t Loss de Treinamento: 0.5430417060852051\n",
      "Epoca 98/500 \t Loss de Treinamento: 0.5089589953422546\n",
      "Epoca 99/500 \t Loss de Treinamento: 0.587255597114563\n",
      "Epoca 100/500 \t Loss de Treinamento: 0.467336505651474\n",
      "Epoca 101/500 \t Loss de Treinamento: 0.5443841814994812\n",
      "Epoca 102/500 \t Loss de Treinamento: 0.47930532693862915\n",
      "Epoca 103/500 \t Loss de Treinamento: 0.4654633402824402\n",
      "Epoca 104/500 \t Loss de Treinamento: 0.46803218126296997\n",
      "Epoca 105/500 \t Loss de Treinamento: 0.45970094203948975\n",
      "Epoca 106/500 \t Loss de Treinamento: 0.465227872133255\n",
      "Epoca 107/500 \t Loss de Treinamento: 0.5783463716506958\n",
      "Epoca 108/500 \t Loss de Treinamento: 0.47541728615760803\n",
      "Epoca 109/500 \t Loss de Treinamento: 0.6257286071777344\n",
      "Epoca 110/500 \t Loss de Treinamento: 0.49794140458106995\n",
      "Epoca 111/500 \t Loss de Treinamento: 0.4694753885269165\n",
      "Epoca 112/500 \t Loss de Treinamento: 0.41684579849243164\n",
      "Epoca 113/500 \t Loss de Treinamento: 0.4645685851573944\n",
      "Epoca 114/500 \t Loss de Treinamento: 0.5023842453956604\n",
      "Epoca 115/500 \t Loss de Treinamento: 0.4760114848613739\n",
      "Epoca 116/500 \t Loss de Treinamento: 0.4307718873023987\n",
      "Epoca 117/500 \t Loss de Treinamento: 0.4341762065887451\n",
      "Epoca 118/500 \t Loss de Treinamento: 0.6154314279556274\n",
      "Epoca 119/500 \t Loss de Treinamento: 0.4934175908565521\n",
      "Epoca 120/500 \t Loss de Treinamento: 0.5346891283988953\n",
      "Epoca 121/500 \t Loss de Treinamento: 0.5486741065979004\n",
      "Epoca 122/500 \t Loss de Treinamento: 0.6278562545776367\n",
      "Epoca 123/500 \t Loss de Treinamento: 0.5404562950134277\n",
      "Epoca 124/500 \t Loss de Treinamento: 0.5317466259002686\n",
      "Epoca 125/500 \t Loss de Treinamento: 0.46541398763656616\n",
      "Epoca 126/500 \t Loss de Treinamento: 0.42850950360298157\n",
      "Epoca 127/500 \t Loss de Treinamento: 0.5416010618209839\n",
      "Epoca 128/500 \t Loss de Treinamento: 0.46632876992225647\n",
      "Epoca 129/500 \t Loss de Treinamento: 0.5288999676704407\n",
      "Epoca 130/500 \t Loss de Treinamento: 0.610887348651886\n",
      "Epoca 131/500 \t Loss de Treinamento: 0.4745929539203644\n",
      "Epoca 132/500 \t Loss de Treinamento: 0.5387260913848877\n",
      "Epoca 133/500 \t Loss de Treinamento: 0.4535839557647705\n",
      "Epoca 134/500 \t Loss de Treinamento: 0.5424017906188965\n",
      "Epoca 135/500 \t Loss de Treinamento: 0.5520381927490234\n",
      "Epoca 136/500 \t Loss de Treinamento: 0.4578856825828552\n",
      "Epoca 137/500 \t Loss de Treinamento: 0.43342360854148865\n",
      "Epoca 138/500 \t Loss de Treinamento: 0.4894794523715973\n",
      "Epoca 139/500 \t Loss de Treinamento: 0.541248619556427\n",
      "Epoca 140/500 \t Loss de Treinamento: 0.5358837842941284\n",
      "Epoca 141/500 \t Loss de Treinamento: 0.5185637474060059\n",
      "Epoca 142/500 \t Loss de Treinamento: 0.5417722463607788\n",
      "Epoca 143/500 \t Loss de Treinamento: 0.4491759240627289\n",
      "Epoca 144/500 \t Loss de Treinamento: 0.5378543138504028\n",
      "Epoca 145/500 \t Loss de Treinamento: 0.47955653071403503\n",
      "Epoca 146/500 \t Loss de Treinamento: 0.47005197405815125\n",
      "Epoca 147/500 \t Loss de Treinamento: 0.5096198320388794\n",
      "Epoca 148/500 \t Loss de Treinamento: 0.6007044911384583\n",
      "Epoca 149/500 \t Loss de Treinamento: 0.4530026316642761\n",
      "Epoca 150/500 \t Loss de Treinamento: 0.4938860833644867\n",
      "Epoca 151/500 \t Loss de Treinamento: 0.506561279296875\n",
      "Epoca 152/500 \t Loss de Treinamento: 0.6130331158638\n",
      "Epoca 153/500 \t Loss de Treinamento: 0.5603014826774597\n",
      "Epoca 154/500 \t Loss de Treinamento: 0.45614156126976013\n",
      "Epoca 155/500 \t Loss de Treinamento: 0.47309610247612\n",
      "Epoca 156/500 \t Loss de Treinamento: 0.5072029232978821\n",
      "Epoca 157/500 \t Loss de Treinamento: 0.4153725504875183\n",
      "Epoca 158/500 \t Loss de Treinamento: 0.5472543835639954\n",
      "Epoca 159/500 \t Loss de Treinamento: 0.44773808121681213\n",
      "Epoca 160/500 \t Loss de Treinamento: 0.43646863102912903\n",
      "Epoca 161/500 \t Loss de Treinamento: 0.43392863869667053\n",
      "Epoca 162/500 \t Loss de Treinamento: 0.4656180441379547\n",
      "Epoca 163/500 \t Loss de Treinamento: 0.4480922520160675\n",
      "Epoca 164/500 \t Loss de Treinamento: 0.4406450688838959\n",
      "Epoca 165/500 \t Loss de Treinamento: 0.46233558654785156\n",
      "Epoca 166/500 \t Loss de Treinamento: 0.4509933292865753\n",
      "Epoca 167/500 \t Loss de Treinamento: 0.5103393197059631\n",
      "Epoca 168/500 \t Loss de Treinamento: 0.43559587001800537\n",
      "Epoca 169/500 \t Loss de Treinamento: 0.5344857573509216\n",
      "Epoca 170/500 \t Loss de Treinamento: 0.3971792161464691\n",
      "Epoca 171/500 \t Loss de Treinamento: 0.4820973873138428\n",
      "Epoca 172/500 \t Loss de Treinamento: 0.3851967453956604\n",
      "Epoca 173/500 \t Loss de Treinamento: 0.48220956325531006\n",
      "Epoca 174/500 \t Loss de Treinamento: 0.563793420791626\n",
      "Epoca 175/500 \t Loss de Treinamento: 0.4939812123775482\n",
      "Epoca 176/500 \t Loss de Treinamento: 0.437796950340271\n",
      "Epoca 177/500 \t Loss de Treinamento: 0.44957125186920166\n",
      "Epoca 178/500 \t Loss de Treinamento: 0.44758033752441406\n",
      "Epoca 179/500 \t Loss de Treinamento: 0.4931039810180664\n",
      "Epoca 180/500 \t Loss de Treinamento: 0.4392923414707184\n",
      "Epoca 181/500 \t Loss de Treinamento: 0.4899302124977112\n",
      "Epoca 182/500 \t Loss de Treinamento: 0.48899951577186584\n",
      "Epoca 183/500 \t Loss de Treinamento: 0.5315278768539429\n",
      "Epoca 184/500 \t Loss de Treinamento: 0.49490317702293396\n",
      "Epoca 185/500 \t Loss de Treinamento: 0.45082640647888184\n",
      "Epoca 186/500 \t Loss de Treinamento: 0.40520399808883667\n",
      "Epoca 187/500 \t Loss de Treinamento: 0.43743449449539185\n",
      "Epoca 188/500 \t Loss de Treinamento: 0.44723737239837646\n",
      "Epoca 189/500 \t Loss de Treinamento: 0.40186113119125366\n",
      "Epoca 190/500 \t Loss de Treinamento: 0.46404537558555603\n",
      "Epoca 191/500 \t Loss de Treinamento: 0.5597679018974304\n",
      "Epoca 192/500 \t Loss de Treinamento: 0.44464588165283203\n",
      "Epoca 193/500 \t Loss de Treinamento: 0.4994250237941742\n",
      "Epoca 194/500 \t Loss de Treinamento: 0.5369581580162048\n",
      "Epoca 195/500 \t Loss de Treinamento: 0.507657527923584\n",
      "Epoca 196/500 \t Loss de Treinamento: 0.6018989682197571\n",
      "Epoca 197/500 \t Loss de Treinamento: 0.5934812426567078\n",
      "Epoca 198/500 \t Loss de Treinamento: 0.4776269495487213\n",
      "Epoca 199/500 \t Loss de Treinamento: 0.49008727073669434\n",
      "Epoca 200/500 \t Loss de Treinamento: 0.5067406892776489\n",
      "Epoca 201/500 \t Loss de Treinamento: 0.41378867626190186\n",
      "Epoca 202/500 \t Loss de Treinamento: 0.4911564290523529\n",
      "Epoca 203/500 \t Loss de Treinamento: 0.4755985736846924\n",
      "Epoca 204/500 \t Loss de Treinamento: 0.5435236096382141\n",
      "Epoca 205/500 \t Loss de Treinamento: 0.5921156406402588\n",
      "Epoca 206/500 \t Loss de Treinamento: 0.4190253019332886\n",
      "Epoca 207/500 \t Loss de Treinamento: 0.4808805286884308\n",
      "Epoca 208/500 \t Loss de Treinamento: 0.49591219425201416\n",
      "Epoca 209/500 \t Loss de Treinamento: 0.372219055891037\n",
      "Epoca 210/500 \t Loss de Treinamento: 0.50882488489151\n",
      "Epoca 211/500 \t Loss de Treinamento: 0.4058559536933899\n",
      "Epoca 212/500 \t Loss de Treinamento: 0.538099467754364\n",
      "Epoca 213/500 \t Loss de Treinamento: 0.4277435541152954\n",
      "Epoca 214/500 \t Loss de Treinamento: 0.4400064945220947\n",
      "Epoca 215/500 \t Loss de Treinamento: 0.5031402707099915\n",
      "Epoca 216/500 \t Loss de Treinamento: 0.49094536900520325\n",
      "Epoca 217/500 \t Loss de Treinamento: 0.4158937335014343\n",
      "Epoca 218/500 \t Loss de Treinamento: 0.4465825855731964\n",
      "Epoca 219/500 \t Loss de Treinamento: 0.4787440598011017\n",
      "Epoca 220/500 \t Loss de Treinamento: 0.4545036256313324\n",
      "Epoca 221/500 \t Loss de Treinamento: 0.4058557152748108\n",
      "Epoca 222/500 \t Loss de Treinamento: 0.48557206988334656\n",
      "Epoca 223/500 \t Loss de Treinamento: 0.4691012501716614\n",
      "Epoca 224/500 \t Loss de Treinamento: 0.43714940547943115\n",
      "Epoca 225/500 \t Loss de Treinamento: 0.4443511962890625\n",
      "Epoca 226/500 \t Loss de Treinamento: 0.39171290397644043\n",
      "Epoca 227/500 \t Loss de Treinamento: 0.4697052836418152\n",
      "Epoca 228/500 \t Loss de Treinamento: 0.40345993638038635\n",
      "Epoca 229/500 \t Loss de Treinamento: 0.4551679193973541\n",
      "Epoca 230/500 \t Loss de Treinamento: 0.4386090636253357\n",
      "Epoca 231/500 \t Loss de Treinamento: 0.47108855843544006\n",
      "Epoca 232/500 \t Loss de Treinamento: 0.4264069199562073\n",
      "Epoca 233/500 \t Loss de Treinamento: 0.48007863759994507\n",
      "Epoca 234/500 \t Loss de Treinamento: 0.5285882353782654\n",
      "Epoca 235/500 \t Loss de Treinamento: 0.40948018431663513\n",
      "Epoca 236/500 \t Loss de Treinamento: 0.45471644401550293\n",
      "Epoca 237/500 \t Loss de Treinamento: 0.4057505130767822\n",
      "Epoca 238/500 \t Loss de Treinamento: 0.4891238510608673\n",
      "Epoca 239/500 \t Loss de Treinamento: 0.4561227858066559\n",
      "Epoca 240/500 \t Loss de Treinamento: 0.37997353076934814\n",
      "Epoca 241/500 \t Loss de Treinamento: 0.40431979298591614\n",
      "Epoca 242/500 \t Loss de Treinamento: 0.41004517674446106\n",
      "Epoca 243/500 \t Loss de Treinamento: 0.4186144769191742\n",
      "Epoca 244/500 \t Loss de Treinamento: 0.37533578276634216\n",
      "Epoca 245/500 \t Loss de Treinamento: 0.3944135308265686\n",
      "Epoca 246/500 \t Loss de Treinamento: 0.47304511070251465\n",
      "Epoca 247/500 \t Loss de Treinamento: 0.4271310865879059\n",
      "Epoca 248/500 \t Loss de Treinamento: 0.38396403193473816\n",
      "Epoca 249/500 \t Loss de Treinamento: 0.4314933121204376\n",
      "Epoca 250/500 \t Loss de Treinamento: 0.45425429940223694\n",
      "Epoca 251/500 \t Loss de Treinamento: 0.36371269822120667\n",
      "Epoca 252/500 \t Loss de Treinamento: 0.43065890669822693\n",
      "Epoca 253/500 \t Loss de Treinamento: 0.4019467830657959\n",
      "Epoca 254/500 \t Loss de Treinamento: 0.4017142057418823\n",
      "Epoca 255/500 \t Loss de Treinamento: 0.48544642329216003\n",
      "Epoca 256/500 \t Loss de Treinamento: 0.4036082923412323\n",
      "Epoca 257/500 \t Loss de Treinamento: 0.37805309891700745\n",
      "Epoca 258/500 \t Loss de Treinamento: 0.40987688302993774\n",
      "Epoca 259/500 \t Loss de Treinamento: 0.3989028334617615\n",
      "Epoca 260/500 \t Loss de Treinamento: 0.44980359077453613\n",
      "Epoca 261/500 \t Loss de Treinamento: 0.4183393716812134\n",
      "Epoca 262/500 \t Loss de Treinamento: 0.36007821559906006\n",
      "Epoca 263/500 \t Loss de Treinamento: 0.37913617491722107\n",
      "Epoca 264/500 \t Loss de Treinamento: 0.4383643865585327\n",
      "Epoca 265/500 \t Loss de Treinamento: 0.4444880187511444\n",
      "Epoca 266/500 \t Loss de Treinamento: 0.43097394704818726\n",
      "Epoca 267/500 \t Loss de Treinamento: 0.35262662172317505\n",
      "Epoca 268/500 \t Loss de Treinamento: 0.43393781781196594\n",
      "Epoca 269/500 \t Loss de Treinamento: 0.38372498750686646\n",
      "Epoca 270/500 \t Loss de Treinamento: 0.3557395040988922\n",
      "Epoca 271/500 \t Loss de Treinamento: 0.44178661704063416\n",
      "Epoca 272/500 \t Loss de Treinamento: 0.39142635464668274\n",
      "Epoca 273/500 \t Loss de Treinamento: 0.38796135783195496\n",
      "Epoca 274/500 \t Loss de Treinamento: 0.3300705552101135\n",
      "Epoca 275/500 \t Loss de Treinamento: 0.3688924312591553\n",
      "Epoca 276/500 \t Loss de Treinamento: 0.38464125990867615\n",
      "Epoca 277/500 \t Loss de Treinamento: 0.4965231418609619\n",
      "Epoca 278/500 \t Loss de Treinamento: 0.3355982005596161\n",
      "Epoca 279/500 \t Loss de Treinamento: 0.33227622509002686\n",
      "Epoca 280/500 \t Loss de Treinamento: 0.3930060863494873\n",
      "Epoca 281/500 \t Loss de Treinamento: 0.4362550675868988\n",
      "Epoca 282/500 \t Loss de Treinamento: 0.36750879883766174\n",
      "Epoca 283/500 \t Loss de Treinamento: 0.3799234926700592\n",
      "Epoca 284/500 \t Loss de Treinamento: 0.4236845076084137\n",
      "Epoca 285/500 \t Loss de Treinamento: 0.3830641508102417\n",
      "Epoca 286/500 \t Loss de Treinamento: 0.36732107400894165\n",
      "Epoca 287/500 \t Loss de Treinamento: 0.3563613295555115\n",
      "Epoca 288/500 \t Loss de Treinamento: 0.42039942741394043\n",
      "Epoca 289/500 \t Loss de Treinamento: 0.3676016926765442\n",
      "Epoca 290/500 \t Loss de Treinamento: 0.3920210599899292\n",
      "Epoca 291/500 \t Loss de Treinamento: 0.3945501744747162\n",
      "Epoca 292/500 \t Loss de Treinamento: 0.3377928137779236\n",
      "Epoca 293/500 \t Loss de Treinamento: 0.3400675654411316\n",
      "Epoca 294/500 \t Loss de Treinamento: 0.37467771768569946\n",
      "Epoca 295/500 \t Loss de Treinamento: 0.444098562002182\n",
      "Epoca 296/500 \t Loss de Treinamento: 0.3729044497013092\n",
      "Epoca 297/500 \t Loss de Treinamento: 0.39664146304130554\n",
      "Epoca 298/500 \t Loss de Treinamento: 0.36592409014701843\n",
      "Epoca 299/500 \t Loss de Treinamento: 0.3404695987701416\n",
      "Epoca 300/500 \t Loss de Treinamento: 0.37081947922706604\n",
      "Epoca 301/500 \t Loss de Treinamento: 0.3759993016719818\n",
      "Epoca 302/500 \t Loss de Treinamento: 0.3694140911102295\n",
      "Epoca 303/500 \t Loss de Treinamento: 0.36897432804107666\n",
      "Epoca 304/500 \t Loss de Treinamento: 0.3815073072910309\n",
      "Epoca 305/500 \t Loss de Treinamento: 0.34886473417282104\n",
      "Epoca 306/500 \t Loss de Treinamento: 0.41391193866729736\n",
      "Epoca 307/500 \t Loss de Treinamento: 0.40146511793136597\n",
      "Epoca 308/500 \t Loss de Treinamento: 0.3503601849079132\n",
      "Epoca 309/500 \t Loss de Treinamento: 0.36583927273750305\n",
      "Epoca 310/500 \t Loss de Treinamento: 0.3940819501876831\n",
      "Epoca 311/500 \t Loss de Treinamento: 0.3549783229827881\n",
      "Epoca 312/500 \t Loss de Treinamento: 0.33256351947784424\n",
      "Epoca 313/500 \t Loss de Treinamento: 0.3260224461555481\n",
      "Epoca 314/500 \t Loss de Treinamento: 0.357878714799881\n",
      "Epoca 315/500 \t Loss de Treinamento: 0.3190597593784332\n",
      "Epoca 316/500 \t Loss de Treinamento: 0.4051705002784729\n",
      "Epoca 317/500 \t Loss de Treinamento: 0.33802902698516846\n",
      "Epoca 318/500 \t Loss de Treinamento: 0.3979155421257019\n",
      "Epoca 319/500 \t Loss de Treinamento: 0.38115447759628296\n",
      "Epoca 320/500 \t Loss de Treinamento: 0.3167593479156494\n",
      "Epoca 321/500 \t Loss de Treinamento: 0.38239023089408875\n",
      "Epoca 322/500 \t Loss de Treinamento: 0.3597392737865448\n",
      "Epoca 323/500 \t Loss de Treinamento: 0.4153206944465637\n",
      "Epoca 324/500 \t Loss de Treinamento: 0.3882994055747986\n",
      "Epoca 325/500 \t Loss de Treinamento: 0.3406025767326355\n",
      "Epoca 326/500 \t Loss de Treinamento: 0.33760809898376465\n",
      "Epoca 327/500 \t Loss de Treinamento: 0.33133020997047424\n",
      "Epoca 328/500 \t Loss de Treinamento: 0.3625444769859314\n",
      "Epoca 329/500 \t Loss de Treinamento: 0.36235225200653076\n",
      "Epoca 330/500 \t Loss de Treinamento: 0.34693461656570435\n",
      "Epoca 331/500 \t Loss de Treinamento: 0.37110480666160583\n",
      "Epoca 332/500 \t Loss de Treinamento: 0.3393878936767578\n",
      "Epoca 333/500 \t Loss de Treinamento: 0.3546408712863922\n",
      "Epoca 334/500 \t Loss de Treinamento: 0.3676948547363281\n",
      "Epoca 335/500 \t Loss de Treinamento: 0.3249800503253937\n",
      "Epoca 336/500 \t Loss de Treinamento: 0.4089953601360321\n",
      "Epoca 337/500 \t Loss de Treinamento: 0.3555644750595093\n",
      "Epoca 338/500 \t Loss de Treinamento: 0.33392849564552307\n",
      "Epoca 339/500 \t Loss de Treinamento: 0.37697768211364746\n",
      "Epoca 340/500 \t Loss de Treinamento: 0.32776108384132385\n",
      "Epoca 341/500 \t Loss de Treinamento: 0.3209281861782074\n",
      "Epoca 342/500 \t Loss de Treinamento: 0.33967962861061096\n",
      "Epoca 343/500 \t Loss de Treinamento: 0.38050737977027893\n",
      "Epoca 344/500 \t Loss de Treinamento: 0.35045742988586426\n",
      "Epoca 345/500 \t Loss de Treinamento: 0.4046080708503723\n",
      "Epoca 346/500 \t Loss de Treinamento: 0.39927589893341064\n",
      "Epoca 347/500 \t Loss de Treinamento: 0.33661019802093506\n",
      "Epoca 348/500 \t Loss de Treinamento: 0.37598586082458496\n",
      "Epoca 349/500 \t Loss de Treinamento: 0.36848118901252747\n",
      "Epoca 350/500 \t Loss de Treinamento: 0.32956841588020325\n",
      "Epoca 351/500 \t Loss de Treinamento: 0.31910941004753113\n",
      "Epoca 352/500 \t Loss de Treinamento: 0.3812108039855957\n",
      "Epoca 353/500 \t Loss de Treinamento: 0.33281731605529785\n",
      "Epoca 354/500 \t Loss de Treinamento: 0.3264259696006775\n",
      "Epoca 355/500 \t Loss de Treinamento: 0.36340561509132385\n",
      "Epoca 356/500 \t Loss de Treinamento: 0.3234770894050598\n",
      "Epoca 357/500 \t Loss de Treinamento: 0.3347817361354828\n",
      "Epoca 358/500 \t Loss de Treinamento: 0.36238789558410645\n",
      "Epoca 359/500 \t Loss de Treinamento: 0.39767834544181824\n",
      "Epoca 360/500 \t Loss de Treinamento: 0.3233392536640167\n",
      "Epoca 361/500 \t Loss de Treinamento: 0.3828222453594208\n",
      "Epoca 362/500 \t Loss de Treinamento: 0.31687453389167786\n",
      "Epoca 363/500 \t Loss de Treinamento: 0.3166077136993408\n",
      "Epoca 364/500 \t Loss de Treinamento: 0.33074140548706055\n",
      "Epoca 365/500 \t Loss de Treinamento: 0.3232291638851166\n",
      "Epoca 366/500 \t Loss de Treinamento: 0.3246011435985565\n",
      "Epoca 367/500 \t Loss de Treinamento: 0.35064056515693665\n",
      "Epoca 368/500 \t Loss de Treinamento: 0.34626439213752747\n",
      "Epoca 369/500 \t Loss de Treinamento: 0.3345806300640106\n",
      "Epoca 370/500 \t Loss de Treinamento: 0.36138343811035156\n",
      "Epoca 371/500 \t Loss de Treinamento: 0.3775325417518616\n",
      "Epoca 372/500 \t Loss de Treinamento: 0.32453668117523193\n",
      "Epoca 373/500 \t Loss de Treinamento: 0.4421570897102356\n",
      "Epoca 374/500 \t Loss de Treinamento: 0.34052759408950806\n",
      "Epoca 375/500 \t Loss de Treinamento: 0.3473232686519623\n",
      "Epoca 376/500 \t Loss de Treinamento: 0.3500707149505615\n",
      "Epoca 377/500 \t Loss de Treinamento: 0.3435499668121338\n",
      "Epoca 378/500 \t Loss de Treinamento: 0.31772366166114807\n",
      "Epoca 379/500 \t Loss de Treinamento: 0.3366909325122833\n",
      "Epoca 380/500 \t Loss de Treinamento: 0.3272189497947693\n",
      "Epoca 381/500 \t Loss de Treinamento: 0.3340598940849304\n",
      "Epoca 382/500 \t Loss de Treinamento: 0.3767395317554474\n",
      "Epoca 383/500 \t Loss de Treinamento: 0.40560397505760193\n",
      "Epoca 384/500 \t Loss de Treinamento: 0.32750022411346436\n",
      "Epoca 385/500 \t Loss de Treinamento: 0.3259260654449463\n",
      "Epoca 386/500 \t Loss de Treinamento: 0.3885563611984253\n",
      "Epoca 387/500 \t Loss de Treinamento: 0.4038121998310089\n",
      "Epoca 388/500 \t Loss de Treinamento: 0.380302369594574\n",
      "Epoca 389/500 \t Loss de Treinamento: 0.38112500309944153\n",
      "Epoca 390/500 \t Loss de Treinamento: 0.3716789186000824\n",
      "Epoca 391/500 \t Loss de Treinamento: 0.36911463737487793\n",
      "Epoca 392/500 \t Loss de Treinamento: 0.3428286612033844\n",
      "Epoca 393/500 \t Loss de Treinamento: 0.3458331525325775\n",
      "Epoca 394/500 \t Loss de Treinamento: 0.31914854049682617\n",
      "Epoca 395/500 \t Loss de Treinamento: 0.34697648882865906\n",
      "Epoca 396/500 \t Loss de Treinamento: 0.3997412621974945\n",
      "Epoca 397/500 \t Loss de Treinamento: 0.4156297743320465\n",
      "Epoca 398/500 \t Loss de Treinamento: 0.32322177290916443\n",
      "Epoca 399/500 \t Loss de Treinamento: 0.33785945177078247\n",
      "Epoca 400/500 \t Loss de Treinamento: 0.3251418173313141\n",
      "Epoca 401/500 \t Loss de Treinamento: 0.35029134154319763\n",
      "Epoca 402/500 \t Loss de Treinamento: 0.3508811891078949\n",
      "Epoca 403/500 \t Loss de Treinamento: 0.3152685761451721\n",
      "Epoca 404/500 \t Loss de Treinamento: 0.3439873158931732\n",
      "Epoca 405/500 \t Loss de Treinamento: 0.34637200832366943\n",
      "Epoca 406/500 \t Loss de Treinamento: 0.3707521855831146\n",
      "Epoca 407/500 \t Loss de Treinamento: 0.32977479696273804\n",
      "Epoca 408/500 \t Loss de Treinamento: 0.36891329288482666\n",
      "Epoca 409/500 \t Loss de Treinamento: 0.3372630774974823\n",
      "Epoca 410/500 \t Loss de Treinamento: 0.38234636187553406\n",
      "Epoca 411/500 \t Loss de Treinamento: 0.35000544786453247\n",
      "Epoca 412/500 \t Loss de Treinamento: 0.3363408148288727\n",
      "Epoca 413/500 \t Loss de Treinamento: 0.3681236803531647\n",
      "Epoca 414/500 \t Loss de Treinamento: 0.32190799713134766\n",
      "Epoca 415/500 \t Loss de Treinamento: 0.31984975934028625\n",
      "Epoca 416/500 \t Loss de Treinamento: 0.3202858567237854\n",
      "Epoca 417/500 \t Loss de Treinamento: 0.3187388479709625\n",
      "Epoca 418/500 \t Loss de Treinamento: 0.31651437282562256\n",
      "Epoca 419/500 \t Loss de Treinamento: 0.454075425863266\n",
      "Epoca 420/500 \t Loss de Treinamento: 0.3353305757045746\n",
      "Epoca 421/500 \t Loss de Treinamento: 0.31908440589904785\n",
      "Epoca 422/500 \t Loss de Treinamento: 0.3248877227306366\n",
      "Epoca 423/500 \t Loss de Treinamento: 0.319306880235672\n",
      "Epoca 424/500 \t Loss de Treinamento: 0.3746289312839508\n",
      "Epoca 425/500 \t Loss de Treinamento: 0.3145957887172699\n",
      "Epoca 426/500 \t Loss de Treinamento: 0.32555827498435974\n",
      "Epoca 427/500 \t Loss de Treinamento: 0.4110598862171173\n",
      "Epoca 428/500 \t Loss de Treinamento: 0.3155752122402191\n",
      "Epoca 429/500 \t Loss de Treinamento: 0.385484516620636\n",
      "Epoca 430/500 \t Loss de Treinamento: 0.3249242603778839\n",
      "Epoca 431/500 \t Loss de Treinamento: 0.32687023282051086\n",
      "Epoca 432/500 \t Loss de Treinamento: 0.3213738203048706\n",
      "Epoca 433/500 \t Loss de Treinamento: 0.358440101146698\n",
      "Epoca 434/500 \t Loss de Treinamento: 0.3560805916786194\n",
      "Epoca 435/500 \t Loss de Treinamento: 0.31380805373191833\n",
      "Epoca 436/500 \t Loss de Treinamento: 0.33655810356140137\n",
      "Epoca 437/500 \t Loss de Treinamento: 0.3325478434562683\n",
      "Epoca 438/500 \t Loss de Treinamento: 0.346822589635849\n",
      "Epoca 439/500 \t Loss de Treinamento: 0.31857308745384216\n",
      "Epoca 440/500 \t Loss de Treinamento: 0.40980491042137146\n",
      "Epoca 441/500 \t Loss de Treinamento: 0.32507044076919556\n",
      "Epoca 442/500 \t Loss de Treinamento: 0.34836873412132263\n",
      "Epoca 443/500 \t Loss de Treinamento: 0.37600764632225037\n",
      "Epoca 444/500 \t Loss de Treinamento: 0.3307362496852875\n",
      "Epoca 445/500 \t Loss de Treinamento: 0.3475469648838043\n",
      "Epoca 446/500 \t Loss de Treinamento: 0.41457393765449524\n",
      "Epoca 447/500 \t Loss de Treinamento: 0.32442009449005127\n",
      "Epoca 448/500 \t Loss de Treinamento: 0.37537112832069397\n",
      "Epoca 449/500 \t Loss de Treinamento: 0.36998817324638367\n",
      "Epoca 450/500 \t Loss de Treinamento: 0.3957763612270355\n",
      "Epoca 451/500 \t Loss de Treinamento: 0.31948161125183105\n",
      "Epoca 452/500 \t Loss de Treinamento: 0.319364458322525\n",
      "Epoca 453/500 \t Loss de Treinamento: 0.38965851068496704\n",
      "Epoca 454/500 \t Loss de Treinamento: 0.35797372460365295\n",
      "Epoca 455/500 \t Loss de Treinamento: 0.3964947760105133\n",
      "Epoca 456/500 \t Loss de Treinamento: 0.32052168250083923\n",
      "Epoca 457/500 \t Loss de Treinamento: 0.31378960609436035\n",
      "Epoca 458/500 \t Loss de Treinamento: 0.3633425533771515\n",
      "Epoca 459/500 \t Loss de Treinamento: 0.3150314390659332\n",
      "Epoca 460/500 \t Loss de Treinamento: 0.3418589234352112\n",
      "Epoca 461/500 \t Loss de Treinamento: 0.3174435794353485\n",
      "Epoca 462/500 \t Loss de Treinamento: 0.3225083649158478\n",
      "Epoca 463/500 \t Loss de Treinamento: 0.39570099115371704\n",
      "Epoca 464/500 \t Loss de Treinamento: 0.3545151352882385\n",
      "Epoca 465/500 \t Loss de Treinamento: 0.4337734282016754\n",
      "Epoca 466/500 \t Loss de Treinamento: 0.31625914573669434\n",
      "Epoca 467/500 \t Loss de Treinamento: 0.3222539722919464\n",
      "Epoca 468/500 \t Loss de Treinamento: 0.31703928112983704\n",
      "Epoca 469/500 \t Loss de Treinamento: 0.3545030653476715\n",
      "Epoca 470/500 \t Loss de Treinamento: 0.32040196657180786\n",
      "Epoca 471/500 \t Loss de Treinamento: 0.3273475766181946\n",
      "Epoca 472/500 \t Loss de Treinamento: 0.3200499415397644\n",
      "Epoca 473/500 \t Loss de Treinamento: 0.3162911832332611\n",
      "Epoca 474/500 \t Loss de Treinamento: 0.3143419325351715\n",
      "Epoca 475/500 \t Loss de Treinamento: 0.361911416053772\n",
      "Epoca 476/500 \t Loss de Treinamento: 0.3209032118320465\n",
      "Epoca 477/500 \t Loss de Treinamento: 0.35118594765663147\n",
      "Epoca 478/500 \t Loss de Treinamento: 0.3495396077632904\n",
      "Epoca 479/500 \t Loss de Treinamento: 0.3148597180843353\n",
      "Epoca 480/500 \t Loss de Treinamento: 0.3561590313911438\n",
      "Epoca 481/500 \t Loss de Treinamento: 0.37140965461730957\n",
      "Epoca 482/500 \t Loss de Treinamento: 0.33363476395606995\n",
      "Epoca 483/500 \t Loss de Treinamento: 0.3691631555557251\n",
      "Epoca 484/500 \t Loss de Treinamento: 0.36497727036476135\n",
      "Epoca 485/500 \t Loss de Treinamento: 0.3687311112880707\n",
      "Epoca 486/500 \t Loss de Treinamento: 0.43492206931114197\n",
      "Epoca 487/500 \t Loss de Treinamento: 0.32612112164497375\n",
      "Epoca 488/500 \t Loss de Treinamento: 0.3235608637332916\n",
      "Epoca 489/500 \t Loss de Treinamento: 0.3144206404685974\n",
      "Epoca 490/500 \t Loss de Treinamento: 0.32346057891845703\n",
      "Epoca 491/500 \t Loss de Treinamento: 0.3210770785808563\n",
      "Epoca 492/500 \t Loss de Treinamento: 0.3348807692527771\n",
      "Epoca 493/500 \t Loss de Treinamento: 0.3175540268421173\n",
      "Epoca 494/500 \t Loss de Treinamento: 0.4017692804336548\n",
      "Epoca 495/500 \t Loss de Treinamento: 0.3150756359100342\n",
      "Epoca 496/500 \t Loss de Treinamento: 0.32148435711860657\n",
      "Epoca 497/500 \t Loss de Treinamento: 0.36838725209236145\n",
      "Epoca 498/500 \t Loss de Treinamento: 0.33747929334640503\n",
      "Epoca 499/500 \t Loss de Treinamento: 0.3426894545555115\n",
      "Epoca 500/500 \t Loss de Treinamento: 0.3197709321975708\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "model.train()\n",
    "for e in range(n_epochs):\n",
    "    loss_total = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        if usar_gpu:\n",
    "            x, y = x.float().cuda(), y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total += loss\n",
    "\n",
    "    loss_total = loss_total / len(ytrain)\n",
    "    print(f\"Epoca {e+1}/{n_epochs} \\t Loss de Treinamento: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9702517162471396\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "loss_test = 0\n",
    "\n",
    "score = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (x, y) in test_loader:\n",
    "        if usar_gpu:\n",
    "            x, y = x.float().cuda(), y.cuda()\n",
    "\n",
    "        eout = model(x)\n",
    "        loss = criterion(eout, y)\n",
    "        loss_test += loss\n",
    "\n",
    "        _, predicted = torch.max(eout.data, 1)\n",
    "\n",
    "        for label, pred in zip(y, predicted):\n",
    "            if label == pred:\n",
    "                score += 1\n",
    "        # print(y, predicted)\n",
    "\n",
    "print(f'acc: {score/len(ytest)}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7288d0bce5149ee2daaefb160e1470a0e67cb78fb53fd4627ee4db88edda5309"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
